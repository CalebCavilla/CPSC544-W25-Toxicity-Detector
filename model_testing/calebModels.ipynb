{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784d1072",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4c8cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Add project root to path (handles __file__ if running as a script or defaults to current directory in notebooks)\n",
    "project_root = Path(__file__).parent.parent if '__file__' in globals() else Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import the custom feature extractor\n",
    "from feature_engineering import ToxicFeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d8aabd",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eef29c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data from ..\\data\\train.csv...\n",
      "Loaded 159571 records with columns: ['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "Splitting data with test_size=0.2, random_state=42...\n",
      "Loading train data from ..\\data\\train.csv...\n",
      "Loaded 159571 records with columns: ['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "Split data into train (127656 records) and validation (31915 records)\n"
     ]
    }
   ],
   "source": [
    "# Initialize feature extractor\n",
    "feature_extractor = ToxicFeatureExtractor(data_dir='../data')\n",
    "\n",
    "# Load the data\n",
    "df = feature_extractor.load_data(dataset='train', verbose=True)\n",
    "if df is None:\n",
    "    raise FileNotFoundError('Training data not found. Please check your data directory.')\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_df, val_df = feature_extractor.split_data(test_size=0.2, random_state=42, verbose=True)\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Extract comment texts and corresponding multi-labels\n",
    "train_comments = train_df['comment_text'].tolist()\n",
    "val_comments = val_df['comment_text'].tolist()\n",
    "\n",
    "train_labels = train_df[label_cols].values\n",
    "val_labels = val_df[label_cols].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91c95da",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4415f80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training features...\n",
      "Beginning feature extraction for 127656 texts...\n",
      "Preprocessing texts...\n",
      "Preprocessing complete.\n",
      "Vectorizing 127656 texts using TfidfVectorizer...\n",
      "Vectorizer parameters: {'max_features': 5000, 'min_df': 2, 'max_df': 0.95, 'ngram_range': (1, 2)}\n",
      "Vectorization complete. Matrix shape: (127656, 5000)\n",
      "Top features: ['10' '100' '1000' '11' '12']...\n",
      "Sample values (first row): [0. 0. 0. 0. 0.]\n",
      "✓ Extracted tfidf features: (127656, 5000)\n",
      "Creating Word2Vec embeddings for 127656 texts...\n",
      "Model parameters: {'vector_size': 100, 'window': 5, 'min_count': 1, 'workers': 4}\n",
      "Embedding complete. Matrix shape: (127656, 100)\n",
      "Sample embeddings (first row, first 5 values): [ 0.12738855  0.61534131 -0.5805735  -0.19541949 -0.68308651]\n",
      "✓ Extracted word2vec features: (127656, 100)\n",
      "Creating Doc2Vec embeddings for 127656 texts...\n",
      "Model parameters: {'vector_size': 100, 'min_count': 2, 'epochs': 40}\n",
      "Embedding complete. Matrix shape: (127656, 100)\n",
      "Sample embeddings (first row, first 5 values): [ 0.15445098 -0.30858997 -0.15388094  0.16160122  0.15133487]\n",
      "✓ Extracted doc2vec features: (127656, 100)\n",
      "Creating FastText embeddings for 127656 texts...\n",
      "Model parameters: {'vector_size': 100, 'window': 5, 'min_count': 1, 'workers': 4}\n",
      "Embedding complete. Matrix shape: (127656, 100)\n",
      "Sample embeddings (first row, first 5 values): [-0.06043139 -0.38108009  0.09737353  0.98010039 -0.72236294]\n",
      "✓ Extracted fasttext features: (127656, 100)\n",
      "Extracting lexical features for 127656 texts...\n",
      "Lexical features complete. Matrix shape: (127656, 10)\n",
      "Sample features (first row): [47.          6.          5.          1.          4.83333333  6.\n",
      "  0.          0.         10.          0.83333333]\n",
      "Feature names: ['char_count', 'word_count', 'unique_word_count', 'sentence_count', 'avg_word_len', 'avg_sentence_len', 'punctuation_count', 'uppercase_count', 'digit_count', 'type_token_ratio']\n",
      "✓ Extracted lexical features: (127656, 10)\n",
      "Extracting POS features for 127656 texts...\n",
      "POS features complete. Matrix shape: (127656, 9)\n",
      "Sample features (first row): [0.66666667 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "Feature names: ['noun', 'verb', 'adj', 'adv', 'pron', 'prep', 'conj', 'det', 'interj']\n",
      "✓ Extracted pos features: (127656, 9)\n",
      "Extracting readability features for 127656 texts...\n",
      "Readability features complete. Matrix shape: (127656, 5)\n",
      "Sample features (first row): [83.62  2.8   6.4   1.59  2.1 ]\n",
      "Feature names: ['flesch_reading_ease', 'flesch_kincaid_grade', 'smog_index', 'coleman_liau_index', 'automated_readability_index']\n",
      "✓ Extracted readability features: (127656, 5)\n",
      "Extracting sentiment features for 127656 texts...\n",
      "Sentiment features complete. Matrix shape: (127656, 4)\n",
      "Sample features (first 3 rows):\n",
      "[[ 0.351   0.      0.649   0.7574]\n",
      " [ 0.037   0.149   0.814  -0.743 ]\n",
      " [ 0.129   0.14    0.731  -0.049 ]]\n",
      "Feature names: ['positive', 'negative', 'neutral', 'compound']\n",
      "✓ Extracted sentiment features: (127656, 4)\n",
      "Extracting topic features for 127656 texts with 10 topics...\n",
      "TF-IDF matrix shape: (127656, 5000)\n",
      "Topic distribution matrix shape: (127656, 10)\n",
      "Sample topic distribution (first row): [0.02828646 0.02826423 0.02826726 0.30540537 0.02826008 0.02826049\n",
      " 0.02827375 0.02826109 0.0282899  0.46843137]\n",
      "Topic #0: dont, im, page, block, know\n",
      "Topic #1: article, game, one, year, list\n",
      "Topic #2: article, page, thanks, talk, added\n",
      "✓ Extracted topic features: (127656, 10)\n",
      "Combined feature matrix shape: (127656, 5338)\n",
      "Sample features (first row, first 10 features):\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Feature types breakdown:\n",
      "  tfidf: (127656, 5000)\n",
      "  word2vec: (127656, 100)\n",
      "  doc2vec: (127656, 100)\n",
      "  fasttext: (127656, 100)\n",
      "  lexical: (127656, 10)\n",
      "  pos: (127656, 9)\n",
      "  readability: (127656, 5)\n",
      "  sentiment: (127656, 4)\n",
      "  topic: (127656, 10)\n",
      "Transforming validation features...\n",
      "Transforming 31915 new texts...\n",
      "Applying TF-IDF vectorization...\n",
      "Extracting Word2Vec features...\n",
      "Creating Word2Vec embeddings for 31915 texts...\n",
      "Model parameters: {'vector_size': 100, 'window': 5, 'min_count': 1, 'workers': 4}\n",
      "Embedding complete. Matrix shape: (31915, 100)\n",
      "Sample embeddings (first row, first 5 values): [-0.37941012 -0.3620055   1.02083004  0.13385145  0.06782729]\n",
      "Extracting Doc2Vec features...\n",
      "Creating Doc2Vec embeddings for 31915 texts...\n",
      "Model parameters: {'vector_size': 100, 'min_count': 2, 'epochs': 40}\n",
      "Embedding complete. Matrix shape: (31915, 100)\n",
      "Sample embeddings (first row, first 5 values): [ 0.06454809 -0.19521359  0.16210218  0.5863234   0.02658075]\n",
      "Extracting FastText features...\n",
      "Creating FastText embeddings for 31915 texts...\n",
      "Model parameters: {'vector_size': 100, 'window': 5, 'min_count': 1, 'workers': 4}\n",
      "Embedding complete. Matrix shape: (31915, 100)\n",
      "Sample embeddings (first row, first 5 values): [-1.10472286  0.2970534  -0.78691554 -0.68975174  0.29406902]\n",
      "Extracting topic features...\n",
      "Extracting lexical features for 31915 texts...\n",
      "Lexical features complete. Matrix shape: (31915, 10)\n",
      "Sample features (first row): [139.          24.          23.           2.           4.08333333\n",
      "  12.           9.           7.           0.           0.95833333]\n",
      "Feature names: ['char_count', 'word_count', 'unique_word_count', 'sentence_count', 'avg_word_len', 'avg_sentence_len', 'punctuation_count', 'uppercase_count', 'digit_count', 'type_token_ratio']\n",
      "Extracting POS features for 31915 texts...\n",
      "POS features complete. Matrix shape: (31915, 9)\n",
      "Sample features (first row): [0.38461538 0.23076923 0.15384615 0.07692308 0.         0.07692308\n",
      " 0.         0.         0.        ]\n",
      "Feature names: ['noun', 'verb', 'adj', 'adv', 'pron', 'prep', 'conj', 'det', 'interj']\n",
      "Extracting sentiment features for 31915 texts...\n",
      "Sentiment features complete. Matrix shape: (31915, 4)\n",
      "Sample features (first 3 rows):\n",
      "[[ 0.345   0.      0.655   0.8402]\n",
      " [ 0.098   0.054   0.849   0.3382]\n",
      " [ 0.078   0.251   0.671  -0.8899]]\n",
      "Feature names: ['positive', 'negative', 'neutral', 'compound']\n",
      "Extracting readability features for 31915 texts...\n",
      "Readability features complete. Matrix shape: (31915, 5)\n",
      "Sample features (first row): [72.5   9.1   0.    6.68 11.6 ]\n",
      "Feature names: ['flesch_reading_ease', 'flesch_kincaid_grade', 'smog_index', 'coleman_liau_index', 'automated_readability_index']\n",
      "Transformation complete. Output matrix shape: (31915, 5338)\n"
     ]
    }
   ],
   "source": [
    "# Extract features for training data\n",
    "print('Extracting training features...')\n",
    "x_train = feature_extractor.extract_all_features(train_comments, verbose=True)\n",
    "\n",
    "# Transform features for validation data\n",
    "print('Transforming validation features...')\n",
    "x_val = feature_extractor.transform_new_data(val_comments, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd4b78",
   "metadata": {},
   "source": [
    "## Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b0de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build logistic regression model within a pipeline\n",
    "base_log = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(max_iter=5000, solver='lbfgs', class_weight='balanced')\n",
    ")\n",
    "model = MultiOutputClassifier(base_log)\n",
    "\n",
    "# Fit the model on the training data\n",
    "print('Training the model...')\n",
    "model.fit(x_train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea02b24c",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8126ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Liam\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Liam\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "base_svm = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVC(kernel='linear', class_weight='balanced', probability=True, max_iter=10000)\n",
    ")\n",
    "\n",
    "model = MultiOutputClassifier(base_svm)\n",
    "\n",
    "print('Training the model...')\n",
    "model.fit(x_train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b9918",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52c2f0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions on validation data...\n",
      "\n",
      "Classification Report for 'toxic':\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00     28869\n",
      "           1       0.10      1.00      0.17      3046\n",
      "\n",
      "    accuracy                           0.10     31915\n",
      "   macro avg       0.55      0.50      0.09     31915\n",
      "weighted avg       0.91      0.10      0.02     31915\n",
      "\n",
      "Accuracy for 'toxic': 0.0957\n",
      "\n",
      "Classification Report for 'severe_toxic':\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.05      0.09     31610\n",
      "           1       0.01      0.88      0.02       305\n",
      "\n",
      "    accuracy                           0.06     31915\n",
      "   macro avg       0.49      0.46      0.05     31915\n",
      "weighted avg       0.97      0.06      0.09     31915\n",
      "\n",
      "Accuracy for 'severe_toxic': 0.0553\n",
      "\n",
      "Classification Report for 'obscene':\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.01      0.02     30253\n",
      "           1       0.05      0.97      0.10      1662\n",
      "\n",
      "    accuracy                           0.06     31915\n",
      "   macro avg       0.47      0.49      0.06     31915\n",
      "weighted avg       0.85      0.06      0.03     31915\n",
      "\n",
      "Accuracy for 'obscene': 0.0615\n",
      "\n",
      "Classification Report for 'threat':\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.10      0.18     31817\n",
      "           1       0.00      0.80      0.01        98\n",
      "\n",
      "    accuracy                           0.10     31915\n",
      "   macro avg       0.50      0.45      0.09     31915\n",
      "weighted avg       0.99      0.10      0.18     31915\n",
      "\n",
      "Accuracy for 'threat': 0.1036\n",
      "\n",
      "Classification Report for 'insult':\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00     30339\n",
      "           1       0.05      1.00      0.09      1576\n",
      "\n",
      "    accuracy                           0.05     31915\n",
      "   macro avg       0.52      0.50      0.05     31915\n",
      "weighted avg       0.95      0.05      0.00     31915\n",
      "\n",
      "Accuracy for 'insult': 0.0494\n",
      "\n",
      "Classification Report for 'identity_hate':\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.02      0.04     31656\n",
      "           1       0.01      0.98      0.02       259\n",
      "\n",
      "    accuracy                           0.03     31915\n",
      "   macro avg       0.50      0.50      0.03     31915\n",
      "weighted avg       0.98      0.03      0.04     31915\n",
      "\n",
      "Accuracy for 'identity_hate': 0.0302\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print('Making predictions on validation data...')\n",
    "predictions = model.predict(x_val)\n",
    "\n",
    "for idx, col in enumerate(label_cols):\n",
    "    print(f\"\\nClassification Report for '{col}':\")\n",
    "    print(classification_report(val_labels[:, idx], predictions[:, idx]))\n",
    "    acc = accuracy_score(val_labels[:, idx], predictions[:, idx])\n",
    "    print(f\"Accuracy for '{col}': {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
